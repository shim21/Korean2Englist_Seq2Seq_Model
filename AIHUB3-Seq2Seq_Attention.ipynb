{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1_구어체(1)_200226.xlsx', '1_구어체(2)_200226.xlsx', '2_대화체_200226.xlsx', '3_문어체_뉴스(1)_200226.zip', '3_문어체_뉴스(2)_200226.xlsx', '3_문어체_뉴스(3)_200226.xlsx', '3_문어체_뉴스(4)_200226.xlsx', '4_문어체_한국문화_200226.xlsx', '5_문어체_조례_200226.xlsx', '6_문어체_지자체웹사이트_200226.xlsx', '불용어.xlsx']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../sugo3/OneDrive/바탕 화면/AIHUB_dataset/\"\n",
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_excel(data_path+\"1_구어체(1)_200226.xlsx\", sheets = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16023</th>\n",
       "      <td>귀사의 웹사이트에 나와있는 주소지로 방문하면 이미 제작되어진 상품을 볼 수 있습니까?</td>\n",
       "      <td>Can I see the products that have already been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53333</th>\n",
       "      <td>나는 솔직히 그들이 3,000달러를 요구하는 것 자체가 이해가 안 가.</td>\n",
       "      <td>I honestly don't understand why they are deman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49634</th>\n",
       "      <td>나는 네가 한 달에 얼마 버는지 궁금해.</td>\n",
       "      <td>I am curious about your monthly salary.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>6 학년은 초등학교에서 가장 높은 학년입니다.</td>\n",
       "      <td>The sixth grade is the highest grade in an ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542</th>\n",
       "      <td>8위는 EXO 멤버였던 크리스입니다.</td>\n",
       "      <td>The 8th place is Chris, a former member of EXO.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   src  \\\n",
       "16023  귀사의 웹사이트에 나와있는 주소지로 방문하면 이미 제작되어진 상품을 볼 수 있습니까?   \n",
       "53333          나는 솔직히 그들이 3,000달러를 요구하는 것 자체가 이해가 안 가.   \n",
       "49634                           나는 네가 한 달에 얼마 버는지 궁금해.   \n",
       "4266                         6 학년은 초등학교에서 가장 높은 학년입니다.   \n",
       "4542                              8위는 EXO 멤버였던 크리스입니다.   \n",
       "\n",
       "                                                     tar  \n",
       "16023  Can I see the products that have already been ...  \n",
       "53333  I honestly don't understand why they are deman...  \n",
       "49634            I am curious about your monthly salary.  \n",
       "4266   The sixth grade is the highest grade in an ele...  \n",
       "4542     The 8th place is Chris, a former member of EXO.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:,\"src\":\"tar\"]\n",
    "lines = lines[0:60000]\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOS, SOS 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32907</th>\n",
       "      <td>그래서 계산이 되지 않았으니 다시 계산해줄게요.</td>\n",
       "      <td>sos So the payment was not made, let me help y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29273</th>\n",
       "      <td>그는 원래 안경을 쓰고 다녀요.</td>\n",
       "      <td>sos He basically wears glasses. eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53432</th>\n",
       "      <td>나는 순식간에 그들의 팬이 되어 버렸어.</td>\n",
       "      <td>sos I became their fan immediately. eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29584</th>\n",
       "      <td>그는 자신이 결근하길 원합니다.</td>\n",
       "      <td>sos He wants to be absent. eos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48885</th>\n",
       "      <td>나는 너를 집까지 데려다줘야 했어.</td>\n",
       "      <td>sos I had to take you to your home. eos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              src  \\\n",
       "32907  그래서 계산이 되지 않았으니 다시 계산해줄게요.   \n",
       "29273           그는 원래 안경을 쓰고 다녀요.   \n",
       "53432      나는 순식간에 그들의 팬이 되어 버렸어.   \n",
       "29584           그는 자신이 결근하길 원합니다.   \n",
       "48885         나는 너를 집까지 데려다줘야 했어.   \n",
       "\n",
       "                                                     tar  \n",
       "32907  sos So the payment was not made, let me help y...  \n",
       "29273                sos He basically wears glasses. eos  \n",
       "53432            sos I became their fan immediately. eos  \n",
       "29584                     sos He wants to be absent. eos  \n",
       "48885            sos I had to take you to your home. eos  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : 'sos '+x+' eos')\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 및 임베딩 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "stopfeatures = ['SF', 'SE', 'SS', 'SP', 'SY', 'SN']\n",
    "stopwords = pd.read_excel(data_path+\"불용어.xlsx\",sheets = 0)\n",
    "stopwords = np.array(stopwords) \n",
    "\n",
    "def preprocessing(text):\n",
    "    preprocessed_text=[]\n",
    "    tokens = mecab.parseToNode(text)\n",
    "    \n",
    "    tokens = tokens.next\n",
    "    \n",
    "    while tokens:\n",
    "        token = tokens.surface\n",
    "        tokens_feature = tokens.feature.split(',')[0]\n",
    "        \n",
    "        if token in stopwords:\n",
    "            tokens = tokens.next\n",
    "            continue                 \n",
    "        \n",
    "        if tokens_feature in stopfeatures:\n",
    "            tokens = tokens.next\n",
    "            continue\n",
    "            \n",
    "        if(tokens_feature=='BOS/EOS'):\n",
    "            break;\n",
    "        else:\n",
    "            #print(token, \"\\t\", tokens_feature)            \n",
    "            preprocessed_text.append(str(token))\n",
    "            tokens = tokens.next        \n",
    "        \n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kor_tokenize(line):\n",
    "    kor = []\n",
    "    for line in lines.src:\n",
    "        tokens = preprocessing(line)\n",
    "        kor.append(tokens)\n",
    "    return kor\n",
    "\n",
    "def eng_tokenize(line):\n",
    "    eng = []\n",
    "    for line in lines.tar:\n",
    "        tokens = word_tokenize(line)\n",
    "        eng.append(tokens)\n",
    "    return eng\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(line): # src와 tar을 읽어와 전처리 시키는 과정\n",
    "    src_t = kor_tokenize(line)\n",
    "    tar_t = eng_tokenize(line)\n",
    "    \n",
    "    input_tensor, inp_lan_tokenizer = tokenize(src_t)\n",
    "    targ_tensor, targ_lan_tokenizer = tokenize(tar_t)\n",
    "    \n",
    "    return input_tensor, targ_tensor, inp_lan_tokenizer, targ_lan_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, input_lan_tokenizer, target_lan_tokenizer = load_dataset(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tensor의 MAX 길이 : 48\n",
      "Kerean Tensor의 MAX 길이 : 34\n"
     ]
    }
   ],
   "source": [
    "print(f\"English Tensor의 MAX 길이 : {max_length_targ}\")\n",
    "print(f\"Kerean Tensor의 MAX 길이 : {max_length_inp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kor train : 48000, Kor val : 12000\n",
      "Eng train : 48000, Eng val : 12000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size = 0.2)\n",
    "print(f\"Kor train : {len(input_tensor_train)}, Kor val : {len(input_tensor_val)}\")  \n",
    "print(f\"Eng train : {len(target_tensor_train)}, Eng val : {len(target_tensor_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print(f\"{t} ---> {lang.index_word[t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to  word mapping\n",
      "2 ---> sos\n",
      "5 ---> i\n",
      "23 ---> was\n",
      "723 ---> born\n",
      "12 ---> in\n",
      "12200 ---> frankfurt\n",
      "8 ---> ,\n",
      "1569 ---> germany\n",
      "3 ---> .\n",
      "1 ---> eos\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language; index to  word mapping\")\n",
    "convert(target_lan_tokenizer, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data 데이터셋 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 32\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_lan_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(target_lan_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 34)\n",
      "(32, 48)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batxh = next(iter(dataset))\n",
    "print(example_input_batch.shape)\n",
    "print(example_target_batxh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING - Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim,enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                     return_sequences = True,\n",
    "                                     return_state = True,\n",
    "                                     recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 34, 1024)\n",
      "(32, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(sample_output.shape)\n",
    "print(sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING - Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis)+self.W2(values)))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis = 1) \n",
    "        \n",
    "        context_vector = attention_weights*values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis =1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1024)\n",
      "(32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(attention_result.shape)\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING - Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru=tf.keras.layers.GRU(self.dec_units,\n",
    "                                    return_sequences = True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        #using Attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x],axis = -1)\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 24699)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE,1 )),\n",
    "                                     sample_hidden, sample_output)\n",
    "print(sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최적화 및 Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction ='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 체크 포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer = optimizer,\n",
    "                                encoder = encoder,\n",
    "                                decoder = decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_lan_tokenizer.word_index['sos']]*BATCH_SIZE,1)\n",
    "        \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:,t], predictions)\n",
    "            \n",
    "            dec_input = tf.expand_dims(targ[:,t], 1)\n",
    "            \n",
    "        batch_loss = (loss/int(targ.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Batch : 0. Loss : 2.620814323425293\n",
      "Epoch : 1, Batch : 100. Loss : 1.9371814727783203\n",
      "Epoch : 1, Batch : 200. Loss : 1.523059606552124\n",
      "Epoch : 1, Batch : 300. Loss : 1.3313764333724976\n",
      "Epoch : 1, Batch : 400. Loss : 1.810246229171753\n",
      "Epoch : 1, Batch : 500. Loss : 1.5048942565917969\n",
      "Epoch : 1, Batch : 600. Loss : 1.2774181365966797\n",
      "Epoch : 1, Batch : 700. Loss : 1.4247686862945557\n",
      "Epoch : 1, Batch : 800. Loss : 1.4597079753875732\n",
      "Epoch : 1, Batch : 900. Loss : 1.272698998451233\n",
      "Epoch : 1, Batch : 1000. Loss : 1.2617205381393433\n",
      "Epoch : 1, Batch : 1100. Loss : 1.222309947013855\n",
      "Epoch : 1, Batch : 1200. Loss : 1.3153375387191772\n",
      "Epoch : 1, Batch : 1300. Loss : 1.272398829460144\n",
      "Epoch : 1, Batch : 1400. Loss : 1.3401367664337158\n",
      "Epoch : 1, Loss : 1.4608796834945679\n",
      "Time taken for 1 epoch 507.9321892261505 sec\n",
      "\n",
      "Epoch : 2, Batch : 0. Loss : 1.2547104358673096\n",
      "Epoch : 2, Batch : 100. Loss : 1.2105871438980103\n",
      "Epoch : 2, Batch : 200. Loss : 1.1035583019256592\n",
      "Epoch : 2, Batch : 300. Loss : 1.1712090969085693\n",
      "Epoch : 2, Batch : 400. Loss : 1.3394559621810913\n",
      "Epoch : 2, Batch : 500. Loss : 1.4481948614120483\n",
      "Epoch : 2, Batch : 600. Loss : 1.2822518348693848\n",
      "Epoch : 2, Batch : 700. Loss : 1.2030748128890991\n",
      "Epoch : 2, Batch : 800. Loss : 1.0757830142974854\n",
      "Epoch : 2, Batch : 900. Loss : 1.1325490474700928\n",
      "Epoch : 2, Batch : 1000. Loss : 1.1771903038024902\n",
      "Epoch : 2, Batch : 1100. Loss : 1.438737154006958\n",
      "Epoch : 2, Batch : 1200. Loss : 1.2158288955688477\n",
      "Epoch : 2, Batch : 1300. Loss : 1.1411728858947754\n",
      "Epoch : 2, Batch : 1400. Loss : 1.1444848775863647\n",
      "Epoch : 2, Loss : 1.2024389505386353\n",
      "Time taken for 1 epoch 472.5520329475403 sec\n",
      "\n",
      "Epoch : 3, Batch : 0. Loss : 0.98297119140625\n",
      "Epoch : 3, Batch : 100. Loss : 1.1055350303649902\n",
      "Epoch : 3, Batch : 200. Loss : 1.0588253736495972\n",
      "Epoch : 3, Batch : 300. Loss : 1.0491840839385986\n",
      "Epoch : 3, Batch : 400. Loss : 1.0919073820114136\n",
      "Epoch : 3, Batch : 500. Loss : 1.1123018264770508\n",
      "Epoch : 3, Batch : 600. Loss : 1.11073899269104\n",
      "Epoch : 3, Batch : 700. Loss : 0.9874557256698608\n",
      "Epoch : 3, Batch : 800. Loss : 1.1845263242721558\n",
      "Epoch : 3, Batch : 900. Loss : 0.8902100324630737\n",
      "Epoch : 3, Batch : 1000. Loss : 1.0849783420562744\n",
      "Epoch : 3, Batch : 1100. Loss : 1.0614873170852661\n",
      "Epoch : 3, Batch : 1200. Loss : 1.1185803413391113\n",
      "Epoch : 3, Batch : 1300. Loss : 1.102537989616394\n",
      "Epoch : 3, Batch : 1400. Loss : 1.096730351448059\n",
      "Epoch : 3, Loss : 1.0351121425628662\n",
      "Time taken for 1 epoch 471.21352314949036 sec\n",
      "\n",
      "Epoch : 4, Batch : 0. Loss : 0.8551660776138306\n",
      "Epoch : 4, Batch : 100. Loss : 0.862329363822937\n",
      "Epoch : 4, Batch : 200. Loss : 0.99828040599823\n",
      "Epoch : 4, Batch : 300. Loss : 0.8978046178817749\n",
      "Epoch : 4, Batch : 400. Loss : 0.8180265426635742\n",
      "Epoch : 4, Batch : 500. Loss : 0.936229944229126\n",
      "Epoch : 4, Batch : 600. Loss : 0.844314455986023\n",
      "Epoch : 4, Batch : 700. Loss : 0.9031579494476318\n",
      "Epoch : 4, Batch : 800. Loss : 1.051316261291504\n",
      "Epoch : 4, Batch : 900. Loss : 0.7383935451507568\n",
      "Epoch : 4, Batch : 1000. Loss : 0.7859073877334595\n",
      "Epoch : 4, Batch : 1100. Loss : 0.9210840463638306\n",
      "Epoch : 4, Batch : 1200. Loss : 0.9660242795944214\n",
      "Epoch : 4, Batch : 1300. Loss : 0.907448947429657\n",
      "Epoch : 4, Batch : 1400. Loss : 0.9221371412277222\n",
      "Epoch : 4, Loss : 0.8833426833152771\n",
      "Time taken for 1 epoch 473.0098707675934 sec\n",
      "\n",
      "Epoch : 5, Batch : 0. Loss : 0.7396640777587891\n",
      "Epoch : 5, Batch : 100. Loss : 0.7039297819137573\n",
      "Epoch : 5, Batch : 200. Loss : 0.9009972810745239\n",
      "Epoch : 5, Batch : 300. Loss : 0.6632730960845947\n",
      "Epoch : 5, Batch : 400. Loss : 0.7555383443832397\n",
      "Epoch : 5, Batch : 500. Loss : 0.8907812833786011\n",
      "Epoch : 5, Batch : 600. Loss : 0.7455527186393738\n",
      "Epoch : 5, Batch : 700. Loss : 0.7007458209991455\n",
      "Epoch : 5, Batch : 800. Loss : 0.627171516418457\n",
      "Epoch : 5, Batch : 900. Loss : 0.7561306357383728\n",
      "Epoch : 5, Batch : 1000. Loss : 0.8189990520477295\n",
      "Epoch : 5, Batch : 1100. Loss : 0.7455524802207947\n",
      "Epoch : 5, Batch : 1200. Loss : 0.5907700061798096\n",
      "Epoch : 5, Batch : 1300. Loss : 0.735643744468689\n",
      "Epoch : 5, Batch : 1400. Loss : 0.742224395275116\n",
      "Epoch : 5, Loss : 0.7340275049209595\n",
      "Time taken for 1 epoch 473.6614534854889 sec\n",
      "\n",
      "Epoch : 6, Batch : 0. Loss : 0.635155200958252\n",
      "Epoch : 6, Batch : 100. Loss : 0.5302673578262329\n",
      "Epoch : 6, Batch : 200. Loss : 0.627409040927887\n",
      "Epoch : 6, Batch : 300. Loss : 0.6211392879486084\n",
      "Epoch : 6, Batch : 400. Loss : 0.5858418941497803\n",
      "Epoch : 6, Batch : 500. Loss : 0.606284499168396\n",
      "Epoch : 6, Batch : 600. Loss : 0.5398721098899841\n",
      "Epoch : 6, Batch : 700. Loss : 0.6220477819442749\n",
      "Epoch : 6, Batch : 800. Loss : 0.7354305982589722\n",
      "Epoch : 6, Batch : 900. Loss : 0.5827170014381409\n",
      "Epoch : 6, Batch : 1000. Loss : 0.6143733859062195\n",
      "Epoch : 6, Batch : 1100. Loss : 0.5034913420677185\n",
      "Epoch : 6, Batch : 1200. Loss : 0.4915883243083954\n",
      "Epoch : 6, Batch : 1300. Loss : 0.6092251539230347\n",
      "Epoch : 6, Batch : 1400. Loss : 0.46536144614219666\n",
      "Epoch : 6, Loss : 0.5933383107185364\n",
      "Time taken for 1 epoch 477.9207742214203 sec\n",
      "\n",
      "Epoch : 7, Batch : 0. Loss : 0.5643676519393921\n",
      "Epoch : 7, Batch : 100. Loss : 0.4314236044883728\n",
      "Epoch : 7, Batch : 200. Loss : 0.46645307540893555\n",
      "Epoch : 7, Batch : 300. Loss : 0.3782742917537689\n",
      "Epoch : 7, Batch : 400. Loss : 0.4175703525543213\n",
      "Epoch : 7, Batch : 500. Loss : 0.41990429162979126\n",
      "Epoch : 7, Batch : 600. Loss : 0.4305051267147064\n",
      "Epoch : 7, Batch : 700. Loss : 0.5056857466697693\n",
      "Epoch : 7, Batch : 800. Loss : 0.46016407012939453\n",
      "Epoch : 7, Batch : 900. Loss : 0.511720597743988\n",
      "Epoch : 7, Batch : 1000. Loss : 0.4744234085083008\n",
      "Epoch : 7, Batch : 1100. Loss : 0.5400645732879639\n",
      "Epoch : 7, Batch : 1200. Loss : 0.41783666610717773\n",
      "Epoch : 7, Batch : 1300. Loss : 0.3570752739906311\n",
      "Epoch : 7, Batch : 1400. Loss : 0.4410896301269531\n",
      "Epoch : 7, Loss : 0.4728122055530548\n",
      "Time taken for 1 epoch 472.91234707832336 sec\n",
      "\n",
      "Epoch : 8, Batch : 0. Loss : 0.296569287776947\n",
      "Epoch : 8, Batch : 100. Loss : 0.3020946979522705\n",
      "Epoch : 8, Batch : 200. Loss : 0.3790757656097412\n",
      "Epoch : 8, Batch : 300. Loss : 0.3455056846141815\n",
      "Epoch : 8, Batch : 400. Loss : 0.35513338446617126\n",
      "Epoch : 8, Batch : 500. Loss : 0.45445483922958374\n",
      "Epoch : 8, Batch : 600. Loss : 0.3151431977748871\n",
      "Epoch : 8, Batch : 700. Loss : 0.3371233642101288\n",
      "Epoch : 8, Batch : 800. Loss : 0.4149586856365204\n",
      "Epoch : 8, Batch : 900. Loss : 0.46309131383895874\n",
      "Epoch : 8, Batch : 1000. Loss : 0.39944905042648315\n",
      "Epoch : 8, Batch : 1100. Loss : 0.4439488351345062\n",
      "Epoch : 8, Batch : 1200. Loss : 0.5652250051498413\n",
      "Epoch : 8, Batch : 1300. Loss : 0.3528333604335785\n",
      "Epoch : 8, Batch : 1400. Loss : 0.3009173572063446\n",
      "Epoch : 8, Loss : 0.37333226203918457\n",
      "Time taken for 1 epoch 474.18520045280457 sec\n",
      "\n",
      "Epoch : 9, Batch : 0. Loss : 0.2325582355260849\n",
      "Epoch : 9, Batch : 100. Loss : 0.2036542147397995\n",
      "Epoch : 9, Batch : 200. Loss : 0.3914242386817932\n",
      "Epoch : 9, Batch : 300. Loss : 0.287288635969162\n",
      "Epoch : 9, Batch : 400. Loss : 0.22365999221801758\n",
      "Epoch : 9, Batch : 500. Loss : 0.278915137052536\n",
      "Epoch : 9, Batch : 600. Loss : 0.2408880889415741\n",
      "Epoch : 9, Batch : 700. Loss : 0.3016830384731293\n",
      "Epoch : 9, Batch : 800. Loss : 0.2790411114692688\n",
      "Epoch : 9, Batch : 900. Loss : 0.3620806336402893\n",
      "Epoch : 9, Batch : 1000. Loss : 0.39231741428375244\n",
      "Epoch : 9, Batch : 1100. Loss : 0.31876951456069946\n",
      "Epoch : 9, Batch : 1200. Loss : 0.2793115973472595\n",
      "Epoch : 9, Batch : 1300. Loss : 0.29554376006126404\n",
      "Epoch : 9, Batch : 1400. Loss : 0.4461291432380676\n",
      "Epoch : 9, Loss : 0.29189369082450867\n",
      "Time taken for 1 epoch 472.9014141559601 sec\n",
      "\n",
      "Epoch : 10, Batch : 0. Loss : 0.2445182502269745\n",
      "Epoch : 10, Batch : 100. Loss : 0.19987736642360687\n",
      "Epoch : 10, Batch : 200. Loss : 0.21882621943950653\n",
      "Epoch : 10, Batch : 300. Loss : 0.22061161696910858\n",
      "Epoch : 10, Batch : 400. Loss : 0.18231932818889618\n",
      "Epoch : 10, Batch : 500. Loss : 0.22882845997810364\n",
      "Epoch : 10, Batch : 600. Loss : 0.195042222738266\n",
      "Epoch : 10, Batch : 700. Loss : 0.2556820213794708\n",
      "Epoch : 10, Batch : 800. Loss : 0.19241464138031006\n",
      "Epoch : 10, Batch : 900. Loss : 0.2578970789909363\n",
      "Epoch : 10, Batch : 1000. Loss : 0.19073468446731567\n",
      "Epoch : 10, Batch : 1100. Loss : 0.3112262189388275\n",
      "Epoch : 10, Batch : 1200. Loss : 0.1831790655851364\n",
      "Epoch : 10, Batch : 1300. Loss : 0.20770621299743652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, Batch : 1400. Loss : 0.24913248419761658\n",
      "Epoch : 10, Loss : 0.2255912870168686\n",
      "Time taken for 1 epoch 475.42292857170105 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch : {epoch+1}, Batch : {batch}. Loss : {batch_loss.numpy()}\")\n",
    "    \n",
    "    if (epoch + 1)%2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        \n",
    "    print(f'Epoch : {epoch+1}, Loss : {total_loss/steps_per_epoch}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence_tokens = preprocessing(sentence)\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    inputs = [input_lan_tokenizer.word_index[i] for i in sentence_tokens]\n",
    "    #print(inputs)\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen = max_length_inp,\n",
    "                                                          padding = 'post')\n",
    "    #print(inputs)   \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_lan_tokenizer.word_index['sos']],0)\n",
    "    #print(max_length_targ)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                            dec_hidden,\n",
    "                                                            enc_out)\n",
    "        \n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += target_lan_tokenizer.index_word[predicted_id]+' '\n",
    "        #print(target_lan_tokenizer.index_word[predicted_id])\n",
    "        if target_lan_tokenizer.index_word[predicted_id] == 'eos':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    print(f'Input : {sentence}')\n",
    "    print(f'Predicted translation : {result[:-4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : 저랑 영화보러 갈까요?\n",
      "Predicted translation : will you go to the movie ? \n"
     ]
    }
   ],
   "source": [
    "translate(u'저랑 영화보러 갈까요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : 바빠요\n",
      "Predicted translation : i am too busy . \n"
     ]
    }
   ],
   "source": [
    "translate(u'바빠요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : 같이 저녁식사는 먹죠?\n",
      "Predicted translation : are we going on the dinner ? \n"
     ]
    }
   ],
   "source": [
    " translate(u'같이 저녁식사는 먹죠?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : 오늘 좀 바쁜일이 있네요\n",
      "Predicted translation : i have been busy today . \n"
     ]
    }
   ],
   "source": [
    " translate(u'오늘 좀 바쁜일이 있네요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : 바쁘다니까\n",
      "Predicted translation : i was so busy , i 've been busy , i 've been busy , i 've been busy , i 've been busy , i 've been busy , i 've been busy , i 've been busy , i 've been busy , i 've b\n"
     ]
    }
   ],
   "source": [
    "translate(u'바쁘다니까')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
